{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jengelwork/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import sys\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "from frameit.corpus import Corpus\n",
    "from frameit.utils import *\n",
    "from frameit.drop_gold_from_train import dropGold\n",
    "from frameit.EvalAFrame import evalFrame\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These notebooks are set up to train a frame that can detect sentences about meals. Feel free to replace the default data files and modify the code as needed to adapt these notebooks to your purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your corpus should be in a .csv file, and the text to be used as training should be in a column titled \"text\", with each data point on a separate line. If you are planning on using a gold set, you should also have an \"Index\" column with id numbers for each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = \"../resources/happy_moment_corpus_small.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: set up a gold set and drop it from the training file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a gold set of positive and negative examples in an XML file, you can drop those examples from\n",
    "the training data with the following code.\n",
    "\n",
    "See the instructions in docs/evaluation.rst for more information on formatting data for the evaluation script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed is  22\n",
      "Indexing data file\n",
      "Recompiling training file\n",
      "file_prefix  \n",
      "Writing gold set to xml file\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#If your positive and negative gold examples are in the same file, you can\n",
    "#pass that file to both parameters–positive examples in the negative file and negative examples in the positive file\n",
    "#will simply be ignored\n",
    "positive_example_file = '../resources/meal_gold_set.xml'\n",
    "negative_example_file = '../resources/meal_gold_set.xml'\n",
    "#Note: for the default data set which is abnormally small, we use a sample size of 5. For your own purposes,\n",
    "#we recommend using at least 100 examples.\n",
    "corpus_file, gold_file = dropGold(corpus_file, positive_example_file, negative_example_file, prefix=\"../resources/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init Corpus\n",
      "Parsing the Semafor data... \n",
      "Parsing the DeepSRL data... \n",
      "Creating Utterances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jengelwork/Documents/FrameItExport/frameit/frameit/corpus.py:54: UserWarning: No FrameNet data found for the corpus.\n",
      "  warnings.warn('No FrameNet data found for the corpus.')\n",
      "/Users/jengelwork/Documents/FrameItExport/frameit/frameit/corpus.py:64: UserWarning: No ProbBank data found for the corpus.\n",
      "  warnings.warn('No ProbBank data found for the corpus.')\n",
      "  0%|          | 0/1989 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the en model\n",
      "\n",
      "    \u001b[93mInfo about spaCy\u001b[0m\n",
      "\n",
      "    spaCy version      2.0.12         \n",
      "    Location           /Users/jengelwork/miniconda3/lib/python3.6/site-packages/spacy\n",
      "    Platform           Darwin-17.7.0-x86_64-i386-64bit\n",
      "    Python version     3.6.1          \n",
      "    Models             en_core_web_md, en_core_web_lg, fr, en, fr_core_news_sm\n",
      "\n",
      "\n",
      "    \u001b[93mInfo about spaCy\u001b[0m\n",
      "\n",
      "    spaCy version      2.0.12         \n",
      "    Location           /Users/jengelwork/miniconda3/lib/python3.6/site-packages/spacy\n",
      "    Platform           Darwin-17.7.0-x86_64-i386-64bit\n",
      "    Python version     3.6.1          \n",
      "    Models             en_core_web_md, en_core_web_lg, fr, en, fr_core_news_sm\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1989/1989 [00:10<00:00, 182.67it/s]\n",
      " 34%|███▍      | 677/1989 [00:00<00:00, 6766.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1989/1989 [00:00<00:00, 3292.97it/s]\n",
      "  0%|          | 8/1989 [00:00<00:24, 79.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing corpus by lemma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1989/1989 [00:22<00:00, 88.63it/s]\n"
     ]
    }
   ],
   "source": [
    "#Corpus data should have one sentence per line in a column titled \"text\". Any other columns will be ignored\n",
    "#When loading a new corpus for the first time, set build_index to True to create indices necessary to process the data.\n",
    "#Otherwise, this step can be safely skipped to significantly speed up runtime by setting build_index to False\n",
    "corpus = Corpus(corpus_file, build_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing a positive set for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A starting point for the positive set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 127 relevant messages in the corpus\n",
      "There are 133 relevant messages in the corpus\n"
     ]
    }
   ],
   "source": [
    "# positive_strings = ['example', 'strings', 'that would be in the', 'positive', 'sentences', 'for', 'the intent',\n",
    "#                    'that you want', 'to extract']\n",
    "positive_strings = ['breakfast', 'brunch', 'lunch', 'dinner']\n",
    "positive_utterances = build_positive_set(corpus, positive_strings)\n",
    "#Note: for exact matches of the strings, use the above function call to build_positive_set(). \n",
    "#To also include matches of all tenses and plural/singular forms of all words in the string, add_lemmas_to_set()\n",
    "lemma_strings = ['restaurant', 'cafe']\n",
    "positive_utterances = add_lemmas_to_set(corpus, lemma_strings, existing_set=positive_utterances)\n",
    "negative_set = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: expand using hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of strings for which no hypernyms were found  0\n",
      "breakfast\n",
      "dinner\n",
      "brunch\n",
      "lunch\n",
      "There are 133 relevant messages in the corpus\n"
     ]
    }
   ],
   "source": [
    "#A hypernym h of a word w is a more generic term that includes w as part of its semantic field. \n",
    "#For example, \"bird\" is a hypernym of \"pigeon\", \"eagle\", \"falcon\", etc. \"Animal\" is a hypernym of \"bird\".\n",
    "\n",
    "#Expanding with hypernyms may not always be appropriate. You may also want to use a different set of terms than \n",
    "#the full list of positive_strings defined earlier\n",
    "\n",
    "positive_utterances = expand_with_hypernym(positive_utterances, positive_strings, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample sentences to check positive set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My daughter and I made dinner together and had meaningful conversation.\n",
      "\n",
      "I had time to get lunch with a friend I had not seen.\n",
      "\n",
      "Cooked myself a wholesome breakfast and drove to work while I watched the sun rise.\n",
      "\n",
      "I had lunch with my girlfriend at a nice restaurant.\n",
      "\n",
      "I had a yummy lunch.\n",
      "\n",
      "We had my favorite meal for dinner.\n",
      "\n",
      "Today on my way to pick up dinner, I heard my current favorite song twice.\n",
      "\n",
      "My son ate dinner with me tonight.\n",
      "\n",
      "I made a dinner of baked ribs and potoatoes with onions.\n",
      "\n",
      "When I grilled dinner for the family and they complimented me\n",
      "\n",
      "I enjoy making new recipes for dinner.\n",
      "\n",
      "I went to my favorite cafe during lunch\n",
      "\n",
      "Got to see my son for lunch.\n",
      "\n",
      "The pizza I had for dinner last night was delicious.\n",
      "\n",
      "When my partner came home from work, it was great to see him and get to eat dinner and laugh together.\n",
      "\n",
      "The fish I caught in the net displayed a fine glimmer in the sun, and became my lunch!\n",
      "\n",
      "My daughter ate all of her dinner including her vegetables.\n",
      "\n",
      "Invited my brother and his family for dinner.\n",
      "\n",
      "My family enjoyed dinner together.\n",
      "\n",
      "I went out for breakfast with my husband today.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a in random.sample(positive_utterances, 20):\n",
    "    print(a.text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trimming the positive set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 relevant messages in the corpus\n"
     ]
    }
   ],
   "source": [
    "#To remove bad examples from the positive set. Also creates a negative set that can optionally be used\n",
    "#Note: there may not necessarily be any bad examples to trim, in which case you should skip this step.\n",
    "remove_list = ['strings', 'that occur', 'in the positive set', 'that correspond', 'to examples',\n",
    "               'that are not positive']\n",
    "positive_utterances, negative_set = trim_examples(positive_utterances, remove_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify hyperparameters for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like, you can customize hyperparameters for the training function. Otherwise, the function will be run with default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_to=700\n",
    "epochs=40\n",
    "batch_size=1400\n",
    "reg_param=0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved info with filename frame_training_info.json.\n"
     ]
    }
   ],
   "source": [
    "# Give the frame a name and save it to a file\n",
    "frame_info_filename = 'frame_training_info.json'\n",
    "frame_name = \"Your Frame Name\"\n",
    "save_frame_training_info_to_file(frame_name, corpus_file, positive_utterances, negative_set,\n",
    "                                scale_to, epochs, batch_size, reg_param, frame_info_filename, gold_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop here and use *Generic lambda_rule attribute exploration.ipynb* and/or *Generic machine-learning attribute exploration.ipynb* if you would like to train attributes for entity-extraction to be used with this frame. When you've collected the necessary data for attributes that you want to train, proceed to the *Train frame* notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
