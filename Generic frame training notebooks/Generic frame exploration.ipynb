{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import sys\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "from frameit.corpus import Corpus\n",
    "from frameit.utils import *\n",
    "from frameit.drop_gold_from_train import dropRepeats\n",
    "from frameit.EvalAFrame import evalFrame\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "
      s": [],
   "source": [
    "corpus_file = \"your_corpus.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you have a gold set of positive and negative examples in an XML file, you can drop those examples from\n",
    "#the training data with the following code. If your positive and negative gold examples are in the same file, you can\n",
    "#pass that file to both parametersâ€“positive examples in the negative file and negative examples in the positive file\n",
    "#will simply be ignored\n",
    "positive_example_file = 'gold_positive.xml'\n",
    "negative_example_file = 'gold_negative.xml'\n",
    "corpus_file, gold_file = dropRepeats(corpus_file, positive_example_file, negative_example_file, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Corpus data should have one sentence per line in a column titled \"text\". Any other columns will be ignored\n",
    "#When loading a new corpus for the first time, set build_index to True to create indices necessary to process the data.\n",
    "#Otherwise, this step can be safely skipped to significantly speed up runtime by setting build_index to False\n",
    "corpus = Corpus(corpus_file, build_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing a positive set for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A starting point for the positive set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_strings = ['example', 'strings', 'that would be in the', 'positive', 'sentences', 'for', 'the intent',\n",
    "                   'that you want', 'to extract']\n",
    "# positive_strings = ['open', 'close', 'when', 'hours', 'late', 'early']\n",
    "positive_utterances = build_positive_set(corpus, positive_strings)\n",
    "#Note: for exact matches of the strings, use the above function call to build_positive_set(). \n",
    "#To also include matches of all tenses and plural/singular forms of all words in the string, add_lemmas_to_set()\n",
    "lemma_strings = ['run', 'dance']\n",
    "positive_utterances = add_lemmas_to_set(corpus, lemma_strings, existing_set=positive_utterances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: expand using hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "
      s": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strings\n",
      "that would be in the\n",
      "positive\n",
      "sentences\n",
      "for\n",
      "the intent\n",
      "that you want\n",
      "to extract\n",
      "Number of strings for which no hypernyms were found  8\n",
      "case\n",
      "example\n",
      "time\n",
      "tasting\n",
      "There are 6003 relevant messages in the corpus\n"
     ]
    }
   ],
   "source": [
    "#A hypernym h of a word w is a more generic term that includes w as part of its semantic field. \n",
    "#For example, \"bird\" is a hypernym of \"pigeon\", \"eagle\", \"falcon\", etc. \"Animal\" is a hypernym of \"bird\".\n",
    "\n",
    "#Expanding with hypernyms may not always be appropriate. You may also want to use a different set of terms than \n",
    "#the full list of positive_strings defined earlier\n",
    "\n",
    "positive_utterances = expand_with_hypernym(positive_utterances, positive_strings, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample sentences to check positive set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in random.sample(positive_utterances, 20):\n",
    "    print(a.text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trimming the positive set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To remove bad examples from the positive set. Also creates a negative set that can optionally be used\n",
    "remove_list = ['strings', 'that occur', 'in the positive set', 'that correspond', 'to examples',\n",
    "               'that are not positive']\n",
    "positive_utterances, negative_set = trim_examples(positive_utterances, remove_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify hyperparameters for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like, you can customize hyperparameters for the training function. Otherwise, the function will be run with default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_to=700\n",
    "epochs=40\n",
    "batch_size=1400\n",
    "reg_param=0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from frameit.train_from_exploration import train_frame_wrapper\n",
    "train_frame_wrapper(\"test_frame.json\", frame_file=frame_filename, ml_attr_files=[\"attr1.json\"], \n",
    "                    lambda_attr_files=[\"attr2.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for testing\n",
    "from frameit import SRL, Frame\n",
    "srl = SRL()\n",
    "frame = Frame.load('test_frame.json')\n",
    "srl.addFrame(frame)\n",
    "srl.parse('Where is the pool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalFrame(frame_filename, gold_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_framers",
   "language": "python",
   "name": "dev_framers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
