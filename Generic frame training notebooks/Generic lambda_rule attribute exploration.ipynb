{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jengelwork/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import sys\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "from frameit.corpus import Corpus\n",
    "from frameit.utils import *\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the positive set of a frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is necessary for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4891 relevant messages in the corpus\n",
      "Loading the en model\n",
      "\n",
      "    \u001b[93mInfo about spaCy\u001b[0m\n",
      "\n",
      "    spaCy version      2.0.12         \n",
      "    Location           /Users/jengelwork/miniconda3/lib/python3.6/site-packages/spacy\n",
      "    Platform           Darwin-17.7.0-x86_64-i386-64bit\n",
      "    Python version     3.6.1          \n",
      "    Models             en_core_web_md, en_core_web_lg, fr, en, fr_core_news_sm\n",
      "\n",
      "\n",
      "    \u001b[93mInfo about spaCy\u001b[0m\n",
      "\n",
      "    spaCy version      2.0.12         \n",
      "    Location           /Users/jengelwork/miniconda3/lib/python3.6/site-packages/spacy\n",
      "    Platform           Darwin-17.7.0-x86_64-i386-64bit\n",
      "    Python version     3.6.1          \n",
      "    Models             en_core_web_md, en_core_web_lg, fr, en, fr_core_news_sm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = 'frame_training_info.json'\n",
    "positive_utterances = load_frame_pos_set(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the part of speech and dependencies that correspond to the attribute you are trying to extract.\n",
    "#You will be able to provide more detailed information to extract attributes later\n",
    "#name: str, the attribute's name, used to identify it\n",
    "#linguistic_info: dict, keys are POS, DEP, and lemma. Values are lists of spacy part-of-speech and dependency tags\n",
    "#(for POS and DEP) and a list of strings for lemma. If values are passed for a key, only attributes matching those\n",
    "#values will be extracted by the model.\n",
    "#unique: bool, if True only one attribute will be extracted per sentence using this model\n",
    "proper_noun_attr = {\"name\": \"Proper Noun Attribute\", \n",
    "                    \"linguistic_info\": {\"POS\": [\"PROPN\"]}, \n",
    "                    \"unique\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative method of attribute extraction: lambda rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambda rules are functions that are added to an attribute and called when a sentence is passed to the frame for classification\n",
    "#They can include any heuristic you want, but need to import any dependencies inside the function\n",
    "#The function is passed to the attribute when calling its training function\n",
    "#(this notebook saves the function to a json file from which the training script can load it)\n",
    "#The following is an example heuristic that extracts proper nouns.\n",
    "def get_prop_name(doc):\n",
    "    from frameit import TextProcessing\n",
    "    tp = TextProcessing()\n",
    "    src_cand = tp.extract_candidates_by_parent(doc, \n",
    "                                               [{}], [{\"pos\":[\"propn\"]}])\n",
    "    src_cand_tokens = []\n",
    "    for (_, sp, sent) in src_cand:\n",
    "        if sp is not None:\n",
    "            for t in sp:\n",
    "                src_cand_tokens.append(t)\n",
    "    return src_cand_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should test your lambda rule function on utterances from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are we able to check in early at all?\n",
      "[]\n",
      "/n\n",
      "Is it possible to get an early Check-in and do you offer free transport from the airport?\n",
      "[]\n",
      "/n\n",
      "Is early check in available?\n",
      "[]\n",
      "/n\n",
      "Do we pay nightly for parking or pay at when we check out?\n",
      "[]\n",
      "/n\n",
      "Is it possible to check in a little early please?\n",
      "[]\n",
      "/n\n",
      "Can we check in early tomorrow?\n",
      "[]\n",
      "/n\n",
      "Is it possible to request early check in?\n",
      "[]\n",
      "/n\n",
      "Do you know when charges will be posted?\n",
      "[]\n",
      "/n\n",
      "recommend a good brunch place close by?\n",
      "[]\n",
      "/n\n",
      "Is parking in the building or close by?\n",
      "[]\n",
      "/n\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    sent = positive_utterances.pop()\n",
    "    print(sent.text)\n",
    "    print(get_prop_name(sent.spacy))\n",
    "    print('/n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the frame and attribute data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved attribute Proper Noun Attribute to file attr2.json\n"
     ]
    }
   ],
   "source": [
    "filename = 'attr2.json'\n",
    "save_lambda_attr_data_to_file(proper_noun_attr, get_prop_name, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
