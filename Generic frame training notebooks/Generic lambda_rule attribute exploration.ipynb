{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the en model\n",
      "\n",
      "    \u001b[93mInfo about spaCy\u001b[0m\n",
      "\n",
      "    spaCy version      2.0.11         \n",
      "    Location           /home/ubuntu/miniconda3/envs/dev_framers/lib/python3.6/site-packages/spacy\n",
      "    Platform           Linux-4.4.0-1049-aws-x86_64-with-debian-stretch-sid\n",
      "    Python version     3.6.5          \n",
      "    Models             en_core_web_lg, en, fr\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import sys\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "from frameit.corpus import Corpus\n",
    "from frameit.utils import *\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the positive set of a frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is necessary for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5951 relevant messages in the corpus\n"
     ]
    }
   ],
   "source": [
    "filename = 'frame_training_info.json'\n",
    "positive_utterances = load_frame_pos_set(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the part of speech and dependencies that correspond to the attribute you are trying to extract.\n",
    "#You will be able to provide more detailed information to extract attributes later\n",
    "#name: str, the attribute's name, used to identify it\n",
    "#linguistic_info: dict, keys are POS, DEP, and lemma. Values are lists of spacy part-of-speech and dependency tags\n",
    "#(for POS and DEP) and a list of strings for lemma. If values are passed for a key, only attributes matching those\n",
    "#values will be extracted by the model.\n",
    "#unique: bool, if True only one attribute will be extracted per sentence using this model\n",
    "proper_noun_attr = {\"name\": \"Proper Noun Attribute\", \n",
    "                    \"linguistic_info\": {\"POS\": [\"PROPN\"]}, \n",
    "                    \"unique\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative method of attribute extraction: lambda rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambda rules are functions that are added to an attribute and called when a sentence is passed to the frame for classification\n",
    "#They can include any heuristic you want, but need to import any dependencies inside the function\n",
    "#The function is passed to the attribute when calling its training function\n",
    "#(this notebook saves the function to a json file from which the training script can load it)\n",
    "#The following is an example heuristic that extracts proper nouns.\n",
    "def get_prop_name(doc):\n",
    "    from frameit import TextProcessing\n",
    "    tp = TextProcessing()\n",
    "    src_cand = tp.extract_candidates_by_parent(doc, \n",
    "                                               [{}], [{\"pos\":[\"propn\"]}])\n",
    "    src_cand_tokens = []\n",
    "    for (_, sp, sent) in src_cand:\n",
    "        if sp is not None:\n",
    "            for t in sp:\n",
    "                src_cand_tokens.append(t)\n",
    "    return src_cand_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should test your lambda rule function on utterances from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can I get a coffee machine for my room?\n",
      "[]\n",
      "/n\n",
      "Are you able to see if there is availability for a 50 minute deep tissue massage in the spa this afternoon?\n",
      "[]\n",
      "/n\n",
      "let me know the password for the wifi?\n",
      "[]\n",
      "/n\n",
      "cancel my room for tonight?\n",
      "[]\n",
      "/n\n",
      "confirm with me what time is check in?\n",
      "[]\n",
      "/n\n",
      "for checkout, do I need to do anything besides leave the key?\n",
      "[]\n",
      "/n\n",
      "Is it possible to get a temporary parking pass (for this evening) ?\n",
      "[]\n",
      "/n\n",
      "Can we request for an early check in?\n",
      "[]\n",
      "/n\n",
      "Do we have any more stationary paper for in room binders?\n",
      "[]\n",
      "/n\n",
      "Is late checkout available for rooom 327?\n",
      "[]\n",
      "/n\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    sent = positive_utterances.pop()\n",
    "    print(sent.text)\n",
    "    print(get_prop_name(sent.spacy))\n",
    "    print('/n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the frame and attribute data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved attribute Proper Noun Attribute to file attr2.json\n"
     ]
    }
   ],
   "source": [
    "filename = 'attr2.json'\n",
    "save_lambda_attr_data_to_file(proper_noun_attr, get_prop_name, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_framers",
   "language": "python",
   "name": "dev_framers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
