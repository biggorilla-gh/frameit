{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import sys\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "from frameit.corpus import Corpus\n",
    "from frameit.utils import *\n",
    "from frameit.utterance import Utterance\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the positive set of a frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should train your attributes based on the same dataset as the frame that they will be attached to. Note that this dataset is of utterances, which contain word-embedding information for an entire sentence. The attribute will be trained based on the individual tokens (representing words) in the sentences. \n",
    "\n",
    "For optimal results, make sure that you have a minimum of 100 positive examples for training your attribute. Depending on how frequently your desired attribute occurs in the dataset, and how unique its grammatical position is compared to other tokens, you may need more examples for desirable results. If the attribute cannot be effectively trained using your available data, we recommend that you attempt to extract it using lambda_rule heuristics. A tutorial for those is available in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'frame_training_info.json'\n",
    "positive_utterances = load_frame_pos_set(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the part of speech and dependencies that correspond to the attribute you are trying to extract.\n",
    "#You will be able to provide more detailed information to extract attributes later\n",
    "#name: str, the attribute's name, used to identify it\n",
    "#linguistic_info: dict, keys are POS, DEP, and lemma. Values are lists of spacy part-of-speech and dependency tags\n",
    "#(for POS and DEP) and a list of strings for lemma. If values are passed for a key, only attributes matching those\n",
    "#values will be extracted by the model.\n",
    "#examples: list, positive examples of the attribute for training. Will be populated later\n",
    "#unique: bool, if True only one attribute will be extracted per sentence using this model\n",
    "attr1 = {\"name\": \"Food\", \n",
    "         \"linguistic_info\": {\"POS\": [\"NOUN\"], \"DEP\":[\"NSUBJ\"]},\n",
    "         \"examples\": list(),\n",
    "         \"unique\": True }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 of extracting attributes: dependency trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find it useful to experiment with Spacy parses here: https://explosion.ai/demos/displacy in order to figure out what dependency constraints to set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part of speech, dependency, and lemma features of the parents of candidate strings\n",
    "dep = [{\"pos\":[\"verb\"], \"lemma\":[\"had\", \"made\", \"ate\", \"eat\", \"make\", \"prepared\", \"cooked\"]}] \n",
    "#part of speech, dependency, and lemma features of the strings themselves\n",
    "cand = [{\"pos\":[\"noun\"], \"dep\":[\"pobj\",\"dobj\",\"ccomp\", \"nsubj\"]}]\n",
    "#extract by \"parent\" or extract by \"child\". Dep constraints will be applied to the specified token\n",
    "dep_type = \"parent\"\n",
    "attr1_candidates = get_attribute_candidates(positive_utterances, dep_type, dep, cand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(attr1_candidates))\n",
    "for e in random.sample(attr1_candidates, 20):\n",
    "    print(e.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 of extracting attributes: list matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, you can implement any heuristic you want to extract positive training data for attributes. Here is a simple example of a heuristic that is applicable in situations where you know that you always want to extract certain terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_list_heuristic(doc):\n",
    "#     target_strings = [\"list\", \"of\", \"strings\", \"to\", \"be\", \"extracted\", \"as\", \"attributes\"]\n",
    "    target_strings = [\"hamburger\", \"breakfast\", \"lunch\", \"dinner\", \"brunch\", \"tea\", \"coffee\", \"pizza\"]\n",
    "    cands = set()\n",
    "    for i in range(len(doc)):\n",
    "        span = doc[i:i+2]\n",
    "        if span.text in target_strings:\n",
    "            for t in span:\n",
    "                cands.add(t)\n",
    "    for token in doc:\n",
    "        if token.text.lower() in target_strings:\n",
    "            cands.add(token)\n",
    "    return list(cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in positive_utterances:\n",
    "    list_extraction_result = simple_list_heuristic(sent.spacy)\n",
    "    if list_extraction_result:\n",
    "        attr1_candidates.update(list_extraction_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(attr1_candidates))\n",
    "for e in random.sample(attr1_candidates, 20):\n",
    "    print(e.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trimming the attribute positive example set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del_list = ['list', 'of', 'strings', 'that', 'should', 'not', 'be', 'extracted', 'as', 'attributes', 'but',\n",
    "#            'are', 'included', 'in', 'the', 'set', 'created', 'by', 'the', 'previous', 'step']\n",
    "del_list = ['husband', 'wife', 'brother', 'sister', 'son', 'daughter']\n",
    "attr1_candidates = remove_attribute_examples(attr1_candidates, del_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(attr1_candidates))\n",
    "for e in random.sample(attr1_candidates, 20):\n",
    "    print(e.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the attribute data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'attr1.json'\n",
    "save_ml_attr_data_to_file(attr1, attr1_candidates, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
